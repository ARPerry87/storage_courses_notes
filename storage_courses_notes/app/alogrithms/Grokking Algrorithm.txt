Binary Search 
	- Binary search is an algorithm and its input is a sorted list of elements, if an element you're looking for is in that list, it returns the position of where it's located, otherwise it returns null. 
		- Binary search starts in the middle, elimenate half the remaining index each time, and each step you eleminate another half until you find the right index/number/string/etc. 
			- Binary search will take log2 n steps to run in the worst case, whereas simple search will take n steps.
			- Binary search only works if the list is sorted

		Binary search in python 

		def binary_search(list, item):
			low = 0
			high = len(list) - 1

			while low <= high:
				mid = (low + high)
				guess = [mid]
				if guess == item: 
					return mid
				if guess  > item: 
					high = mid - 1 
				else: 
					low = mid + 1 
			return None
		my_list = [1, 3, 4, 7, 9]

		print binary_search(my_list, 3) - 1
		print binary_search(my_list, -1) - None

		- Binary search in a list of 100 would only take 7 guesses at most 
		- 4 billion it would take at most 32

- Big O Notation 
	- Tells you how fast an algorithm is 
	- Big O lets you compare the number of operations. It tells you how fast the algorithm grows. 
		- O(n) - what big o notation looks like, it's O (log n) aka O (number of operations)
		- Big O establishes worst case run time
		- O(n) is reassurance because it will never be more than having to look at every single entry 

		Big O times sorted fastest to slowest: 
			- O(log n) - also known as log time: example Binary Search 
			- O(n) - linear search, search every thing
			- O(n^2) - Example: slow sorting algortim, like selection sort 
			- O(n!) - really slow algorithm, like traveling salesperson 
			- O(n*Log n) a fast sorting algorithim like quick sort

			- Algorithm speed isn't measured in seconds, but the growth of number of operations 
			- How quickly the run time of an alogrithm increases as the size of input increases 
			- O(log n) is faster than O(n)

- Arrays - can run out of storage, and will have to add space on the fly, this takes up memory, and you may end up not needing that much memory. Arrays you know the addresses for every item. 
	- harder to insert in middle as it involves shifting
	- Used more often 
	- can allow random access

- Linked List - items can be anywhere in memory, each item stores the address of the next item in the list. A bunch of random memory addresses are linked together. Better for inserts. Addresses are random. 
	- easier to insert into the middle of a linked list as changing the previous elements point to 
	- easier to delete because you just need to change what the previous element points to 
	- Only allow sequential access
- Selection Sort - sort a list and find a specific value and add to a new list, and keep iterating through, selecting a new specific value, adding it to that new list, until the new list is complete and is sorted. 
	- Selection sort has a Big O notation of 
		- O(n^2) as you have to touch every index of a list, and have to do so iteratively, as in multiple times, to get the sorted list. 
		- Can be used to sort: 
			- Names in a phone book 
			- Travel dates 
			- Emails 
		- Quicksort is faster than Selection Sort 
	Example Code 

	def findSmallest(arr)
		smallest = arr[0] # stores the smallest value
		
		smallest_index = 0 # stores the index of the smallest value 
		
		for i in range(1, len(arr)): #starts a for loop that will return the smallest index in an array 
			if arr[i] < smallest: 
				smallest = arr[i]
				smallest_index = i
		return smallest_index 

	def selectionSort(arr):
		newArr = []
		for i in range(len(arr)): #creates a for loop so that the for loop inside of findSmallest will then begin the newArr adding and popping process, to add the smallest element to the array. 

			smallest = findSmallest(arr) #finds the smallest element in the array and ads it to the new array 

			newArr.append(arr.pop(smallest))

		return newArr

	print selectionSort([5, 3, 6, 2, 10])


	- Chapter 3 - Recursions 

	- If given a box to look through until you find something

	code example 

	def look_for_key(main_box):
		pile = main_box.make_a_pile()
		while pile is not empty: 
				box = pile.grab_a_box()
				for item in box: 
					if item.is_a_box():
						pile.append(item)
					elif item.is_a_key():
						print "found the key!"

	Second example is recursion: 

	def look_for_key(box): 
		for item in box: 
			if item.is_a_box: 
				look_for_key(item)
			elif item.is_a_key():
				print("found the key!")

	both approaches accomplish the same thing but the second one is clearer in how it does it. 

	Recursion needs a base case or it will perpetully recurse forever. Example 

	def countdown(i): 
		print i 
		if i <= 0: 
			return 
		else: 
			countdown(i-1)

	The Call Stack 
		- Example of a call stack 
			- A todo list that can only have things added or removed, this is an example of a pop or a push function

		- Code example for a call stack for a computer 

		def greet(name): 
			print "hello, " + name + "!"
			greet2(name)
			print "getting ready to say bye..."
			bye()

		def greet2(name): 
			print"how are you, " + name + "?"

		def bye():
			print "ok bye!"

	When you call a function from another function, the calling function is paused in a partially completed state. All the values of the variables for that function are still stored in memory. 		

	When you're done with one function (as in calling the completed function) in a call stack, like a box with multiple stacks on top of each other, the completed box of information gets popped off, leaving only the partially completed, needing to be finished functions in memory functions left in the stack. 

	Recursive functions use call stack as well. (thank god) Let's us the factorial function with it. Factorial(5) is written as 5! and ti's defined like this 5! = 5 * 4 * 3 * 2 * 1 here's the code for it 

	def fact(x)
		if x ==1
			return 1 
		else: 
			return x * fact(x-1)

	This callstack iterates through the stack of fact, going through twice until it hits x = 1 until it stops as the recurisve call, then stops and popping off the stack, which means its the first call we return from, and returns a 1. The others, return x*x-1 so 3*2

	Stacks play big parts in recursion. 

	Look into tail recursion? 

	- Recursion is when a function calls itself 
	- Every recursive function has two cases: the base case and the recursive case 
	- A stack has two operations: push and pop 
	- All function calls go onto the call stack 
	- The call stack can get very large which takes up a lot of storage 


Chapter 4 - QuickSort 

	- It's a D&C (divide and conquer) strategy
	- It's recurisve in nature 
	- There are two steps 
		- Figure out the base case, this should be the simplest possible case 
		- Divide or decrease your problem until it becomes the base case 

		- Farmer's land problem, find the biggest chunk of the land that can be split up easily by having two boxes that can multiply by each other. The smaller portion of the land left over, can be used to repeat this process. 

		- Euclid's algorithm 
			- If you find the biggest box that will work for this size, that will be the biggest box that will work the entire farm. 

		-you keep dividing the land over and over until you reach the smallest plot of land which will end up being 80m as it's a factor of 160 

	- Recap on how D&C works 
		- Figure out a simple case as the base case 
		- Figure out how to reduce your problem to get to the base case 

		It's a way to think about a problem, not a simple algorithm you can apply to the problem 

		Example of recursion for adding an array on itself from internet: 

		def listsum(numList):
   			if len(numList) == 1:
        		return numList[0]
   			else:
        		return numList[0] + listsum(numList[1:])

		listsum([1,3,5,7,9]))		

	When writing a recursive function involving an array, the base case is often an empty array or an array with one element. If your stuck, try that first. 

	Binary search is also a divide and conquer algorithm. 

	Quicksort 

	- quicksort is a sorting algorithm 
	- much faster than selection sort 
	- No need to sort arrays that are empty or have one element 

	def quicksort(array): 
		if len(array) <2: 
			return array 

	- Array with two elements, if the first element is smaller, if it isn't, swap them. 

	- Three element arrays 

	- First element in an array is called the pivot, you sort on the pivot finding numbers smaller than it, and creating a subarray 

	quicksort([15,10]) + [33] + quicksort([])

	[10, 15, 33]

	- works with any pivot. 

	- both sub-arrays have only one element, and you know how to sort those. 

	- pick a pivot 
	- partition the array into two sub-arrays: elements less than the pivot and elements greater than the pivot
	- call quicksort recursively on the two sub-arrays 

	- if it has four elements, you pivot twice, so that it'll have two pivot points 

	[33, 10, 15, 7]

	[10, 15, 7] [33] []

	[10, 15, 7] [33] []

	[7] [10] [15]

	quicksort([7, 10, 15,]) + [33] + quicksort([])

	[7,10,15,33]

	code for quicksort 

	def quicksort(array):

		if len(array) <2: # base case: arrays with 0 or 1 element are already sorted
			return array
		else: 
			pivot = array[0] # recursive case 
			less = [i for i in array[1:] if i <= pivot] # sub-array of all the elements less than the pivot

			greater = [i for i in array[1:] if i > pivot] # sub-array of all the elements greater than the pivot 

			return quicksort(less) + [pivot] + quicksort(greater) # appending the arrays together to form the sorted array

	print(quicksort([10,5,2,3]))



	Big O notation revisited 

	Merge sort is always O(n log n) whilse quicksort is mostly O(n log n) and is sometimes O(n^^2) so sometimes quicksort is slow. Why shouldn't we always use merge sort?

	- Usually ignore the constant at the beginning of a formula for the Big O notation because it doesn't make much difference when doing calculations when iterating through large datasets. 
		- But sometimes the Constant can make a difference. 
		
		- Quicksort versus Mergesort is an example of the constant making a difference 
		
		- Quicksort has a smaller constant than mergesort, so if they're both O(n log n) Quicksort will be faster

		- Quicksort also hits average case way more often than worst case. 

	- Average case vs worst case 

		- worst case scenario for quicksort is to pivot on the first element, it's better to pick somewhere in the middle as the partition 

		- best case scenario is also the average case 

		- one of the fastest, also very good example of divide and conquer 


	- Exercises 

		- 4.5 

			- it would take O(n) amount of time to print each element in an array as it has to touch every item to print it. 

		4.6 
			- Doubling the value of an array would probably be an example of O(n) as you want to go through and double everything 

		4.7
			- it would be an exmple of O(1) since you're just selecting one thing 

		4.8 
			- O(n2) 
			- creating a multiplication table with all the elements in the array so if your array is 2, 3, 7, 8, 10, you first multiply every element by 2, then by 3, then by 7, and so on. 

			def multiplicationTable (arr, multi, index=1):

			if len(arr) <= 2: 
				return arr

			else: 
				multi = multi * arr[index]
				return multiTable(arr, multi, index+1)
			return [multi*val for val in arr]

			print(multi([2,3,7,8,10], 2))


Chapter 5 

- Finding an item explicitly by it's name or rather it's key value pair, and only that one specific item is in Big O notation terms: 
	- O(1) time 

	- Hash functions map strings to numbers and require a few things: 
		- It needs to be consistent, every time you write in a specific key, you need to get back the same value 
		- Different keys should return different values, no two keys should be the same 
		- Hash function consitently maps a name to the same index. Every time you type a key it'll get the same number back. 
		- The hash fucntion maps different strings to different indexes. Everything maps to different slots in the array to where you put can store it's value. 
		- The hash function knows how big your array is and only returns valid indexes. Won't return larger indexes if your array is only 5, and you ask for 10. 

		example code: 

		book = dict()

		book["apple"] = 0.67
		book["milk"] = 1.49
		book["avocado"] = 1.49
		print(book)

		('avocado': 1.49, 'apple': 0.67, 'milk': 1.49)


		- print book['avocado']
			1.49 #price of the avocado 

		Exercises: 

		 5.1 is consistent

		 Not 

		 Not 

		 Consistent - ok I can see why because it would always return the length you give it and then index it, it's not a random number, and it doesn't give you the next empty slot, so it would be consistent 

	- phone books are great use cases for hash tables 

	- Also use hash mapping for DNS resolution. 

	- They also prevent duplicate entries. 

	example code for if someone has voted or not: 

	voted = {} 

	def check_voter(name): 
		if voted.get(name): 
			print("kick them out"): 
		else: 
			voted[name] = True 
			print("let them vote!")


	By storing these in a hash, you're looking them up by using O(1) which is a much faster way to search than O(n) or O(n^^2). 

	Hashing can also act as caching, similar to memorizing a fact that a child keeps asking. It stops doing all the work of what the homepage looks like, and instead memorized it, and sends a homepage to you on loadup. 

		- Advantages are you get the webpage faster and webpages do less work 

	- All pages are cached and then are mapped to each other through the hashes 

	- example code: 

	cache = {}

	def get_page(url): 
		if cache.get(url):
			return cache[url] # returns cached data
		else: 
			data = get_data_from_server(url)
			cache[url] = data # saves the data in your cache
			return data 

	- here we make the server do work only if the url isn't in the cache. Before you return the data, though, you save it in the cache. 

	- Hashes are good for 
		- Modeling relationships from one thing to another thing 
		- Filtering out duplicates 
		- caching/memorizing data instead of making your server do work 

	- Collisions 

		- Collisions happen when you've already assigned a position in a hash and need to assign that position to another key value pair. 
		- A work around is a linked list in that slot 
		- However, this can quickly become problematic and messy, so ideally work with a function that will evenly distribute your key value pairs 

	- Performance 
		- To avoid collisions that slow down hash tables, you need to avoid low load factor and have a good hash function 
		- Once your load factor is greater than 0.7 it's time to resize your hash table 


Chapter 6

	 - Graph algorithms 
	 - Breadth first search allows you to find the shortest distance between two things but shortest distance can mean a lot of things. 

	 Can be used to: 

	 - Write a checkers AI to write fewest moves to victory 

	 - Spell checker 

	 - Breadth First has two steps: 
	 	- model the problem as a graph 
	 	- solve the problem using breadth first search 

	 - What is a graph 
	 	- A graph is a set of connections. 
	 		- Each Graph is made up of nodes and edges 
	 		- A node can be directly connected to many other nodes, and those nodes are called it's neighbors. 
	 		- A neighbor is only a neighbor if it's directly connected to another node through an edge 
	 		- Graphs are a way to model how different things are connected to one another. 

	 - Helps answers 2 types of questions: 
	 	- Is there a path from node a to b 
	 	- what is the shortest path from a to b 

	 	E.G. searching your friends, then your friends friends, then their friends friends, until you find someone to sell your mangos if you grow mangos, until you exhaust your network. 


	 - Finding the shortest path 

	 - recap

	 	- two questions 
	 		- Is there a path between node a to node b 
	 		- what is the shortest path from node a to node b 

	 -	Breadth first searches first degree connections first before moving on to second degree. 

	 - Queues 
	 	- Can't access random elements, instead there are two only operations, enqueue and dequeue (works similarly to a stack)
	 	- If you enqueue two items to the list the first item you added will be dequeued before the second item. 
	 	- THe quque is called a Fifo data structure. First in, First Out. 
	 	A stack is called LIFO, Last in First Out. 

	 	Exercises: 

	 	6.1 

	 	2 

	 	6.2 

	 	2 


	- Implementing the Graph 

		- A hash table lets you express relationships 

		graph = {}
		graph ["you"] = ["alice", "bob", "claire"]

		- When values have arrows pointing to them, but no arrows from them to another neighbor, we call it a directed graph, the relationship is one way. 

		- An undirected graph doesn't have arrows and both nodes are each others neighbors. 

		Example Code: 

		def search(name): 

			search_queue = dequeue()
			search_queue += graph[name]
			searched = [] #this array is how you keep track of which people you've searched before

			while search_queue: 
				person = search_queue.popleft()
				if not person in searched:  # only search this person if you haven't already searched them 
					if person_is_seller(person):
						print(person + "is a mango seller!")
						return True
					else: 
						search_queue += graph[person]
						searched.append(person) #mark this person as searched 
			return False
		search("you")


	Big O notation for this is O(number of edges) at least. You also keep a queue of every person you search, adding a person takes constant time, so it's O(1). Doing this for every person will take O(number of people). 

	For Breadth First Big O is commonly written as 
		- O(V+E) verticals and edges 


Chapter 7 

Dijkstra's algorithm 

	- Dijkstra's algorithm has 4 steps 
		- Find the cheapest node. This is the node you can get in the leas amount of time. 
		- Check whether there's a cheaper path to the neighbors of this node. If so, update their costs. 
		- Repeat until you've done this for every nodei n the graph. 
		- Calculate the final path. 

		- When you work with Dijkstra's algorithm, each edge has a weight. A graph with weights, is a weighted graph, while one without weights is an unweighted graph. 

		- Unweighted graphs use breadth first search. 

		- Weighted graphs use Dijkstra's 


		- Using this algorithm you follow along the weighted edges to find the shortest path, this however doesn't always mean the aboslutely shortest as with unweighted graphs, as it's attached to numbers. 
			- This typically more has to do with the shortest path in relation to the shortest path in relation to say how fast can you get to work along a route or trading for a piano

		- Can't use Dijkstra's algorithm if you have negative weights it breaks the algorithm 

		The way to find the shortest with weighted negatives with the Bellman-Ford algorithm, but it's out of scope for the book. 


		- Code Examples 

		graph["start"] = {}
		graph["start"]["a"] = 6
		graph["start"]["b"] = 2

		graph["a"] = {}

		graph["a"]["fin"] = 1

		graph["b"] = {}
		graph["b"]["a"] = 3
		graph["b"]["fin"] = 5

		graph["fin"] = {} #the finish node doesn't have any neighbors

		- now you need a hash table to store the cost for each node 
			- The cost of a node is how long it takes to get to that node from the start
			- you don't know how long it takes to get to the finish so you put down ininity  


				infinity = float(“inf”)
				costs = {}
				costs[“a”] = 6
				costs[“b”] = 2
				costs[“fin”] = infinity

		- we also need a hash table for the parents 

		parents = {}
		parents[“a”] = “start”
		parents[“b”] = “start”
		parents[“fin”] = None

		Finally need an array to keep track of all the nodes we've already proccessed so we don't process a node more than once

		processed = []


	Full Code: 

	node = find_lowest_cost_node(costs) #find the lowest cost node that you haven't processed yet 

	while node is not None: # if you've processed all the nodes this loop is done 

		cost = costs[node] 
		neighbors = graph[node] 
		for n in neighbors.keys():# go through all the neighbors of this node 

			new_cost = cost + neighbors[n] 
			if costs[n] > new_cost: # if it's cheaper to get to this neighbor by going through this node 

				costs[n] = new_cost# update the cost for this node 

				parents[n] = node #this node becomes the new parent for this neighbor 

		processed.append(node) #mark the node as processed

		node = finde_lowest_cost_node(costs) #find the next node to process and loop

		Now for the function version: 

		def find_lowest_cost_node(costs): 
			lowest_cost = float("inf")
			lowest_cost_node = None 
			for node in costs: # go through each node
				cost = costs[node]
					if cost < lowest_cost and node not in processed: # if it's the lowest cost so far and hasn't been processed yet

						lowest_cost = cost #set it as the new lowest cost node

						lowest_cost_node = node 
			return lowest_cost_node 


		- maximum time of D algorithm is n^^2 

		- perform relaxation on edges, when you are on one edge, look at other edges, which edge is smaller, relax it 
			- Only relax it (update it) if it's shorter 
			- Using the table to update for the shortest path for updating the smallest vertex 

		- it's a greedy algorithm and it touches everything 

		- Unnecessary processing sometimes 



Chapter 8 

- Greedy algorithms 
	- They're easy, each step they pick the optimal move 
	- Sometimes perfect is the enemy of good enough 
	- Approximation algorithm 
		- pick the station that covers most states that haven't been covered, it's ok if a station covers states that have been covered already 
		- repeat until all states are covered 

		- When calculating an exact solution, an approximation solution will do 

		- Judged by how fast they are, how close they are to the solution 

		- A set union means combine both sets 
		- a set intersection means find the items that show up in both sets 
		- a set difference means subtract the items in one set from the items in the other set 

		fruits = set(["avocado", "tomato", "banana"])
		veggeies = set(["beets", "carrots", "tomato"])
		fruits | veggies #this is a set union 
		set(["avocado", "beets", "carrots", "tomato", "banana"])
		fruits & veggies # this is a set intersection 
		set(["tomato"])
		fruits - veggies # this is a set difference 
		set(["avocado", "banana"])
		veggies - fruits 
		set(["avocado", "banana"])

	code for loop for looking for station with a greedy algorithm: 

	states_needed = set(["mt", "wa", "or", "id", "nv", "ut", ca], "az")
	stations = {}
	stations["kone"] = set(["id", "nv", "ut"])
	stations[“ktwo”] = set([“wa”, “id”, “mt”])
	stations[“kthree”] = set([“or”, “nv”, “ca”])
	stations[“kfour”] = set([“nv”, “ut”])
	stations[“kfive”] = set([“ca”, “az”])
	final_stations = set()

	

	Full code of the while loop: 

	while states_needed:
		best_station = None
		states_covered = set()
		for station, states in stations.items():
			covered = states_needed & states
			if len(covered) > len(states_covered):
				best_station = station
				states_covered = covered
	states_needed -= states_covered
	final_stations.add(best_station)


	- NP complete problems 

	- Traveling salesman, adding more edges increases the routes by magnitudes. It's a factorial function. 
	- NP problems are traveling sales person problems 

	- NP Complete to 
		- find something that fits most of the criteria of a problem 
		- repeat until the solution is fulfilled 

	- Ways to tell the difference if it's NP Complete 
		- Slows down with lots of items 
		- All combinations of X 
		- Do you have to calculate every possible version of x 
		- Involves a sequence 
		- Your problem involves a set and it's hard to solve 
		- Can you restate your problem as the set covering problem or the traveling sales person problem? 

	Exercises 
		- 8.6 
			No it's just a Breadth First Algorithm 
		- 8.7 
			- Yes
		- 8.8 
			- Yes? 

	Recap 
		- Greedy algorithms optimize locally, hoping to end up with global optimum 
		- NP-complete problems have no known fast solution 
		- If you have an NP-complete problem your best bet is to use an approximation algorithm 
		- Greedy algorithms are easy to write and fast to run, so they make good approximation algorithms 


Chapter 8 

Dynamic Programming 

The knapsack problem 
	- If you were to calculate the problem by touching each thing, and trying out each combo it would take O(2^^n) which is horrendous. Enter in dynamic programming. 

		- Dynamic programming starts by solving subproblems and building up to solving the larger problem. 

		For the knapsack problem, you start by solving the problem for smaller knapsacks and then working up to solving the original issue. 

		- Every dynamic programming algorithm starts with a grid. for the knapsack problem it has the rows for the items and the columns for the pounds the knapsack can hold. 
		- The grid starts out empty and fill each cell of the grid 

		- You fill in the grid with each time an item fits the description of the problem-for the example, the guitar is one pound, so it'll fit every box since the other items aren't available at the moment 

		- At every row you can steal the item in that row, or the items in the rows above it, you can't choose to steal the laptop right now, but you can choose to steal the steroe and or the guitar. 
			- the stereo is heavier than one pound, so it doesn't fit into the first slot, so the value of the guitar, and the guitar itself, gets pulled down into the stereo slot of the graph. This happens for the next two slots where the maximum weight doesn't match the weight of the stereo 
			- Keep going until the stereo fits the critera to become the largest, which fills the fourth box. 
			- This process repeats with every item and column until the entire grid is filled in
			- where it gets interesting is that for the 4lbs, instead of bringing just the stereo down, you have 4 lbs: 
				- The guitar is 1lbs, and the laptop is 3lbs, and their combined value is more than the stereo 
		- Max value can never decrease as we're sorting the current max estimate, the estimate can never get worse than it was before. 

		- Order of columns vs rows doesn't make a difference for the knapsack problem but it does for others 
	- This method cannot handle fine granularity, you either take the item or you don't. 
	- Solve instead with a greedy algorithm 
	- DYnamic programming only works when each subproblem is discrete, when it doesn't depend on other subproblems 

	- Longest common substring 
		- Dynamic programming is useful when your trying to optimize something given a constraint 
		- Every dynamic solution involves a grid 
		- values in the cells are usually what you're trying to optimize 
		- each cell is a subproblem 

		- Typing a word and mispelling it, which word would come closest to the word that someone meant to put?

		- What are the value of the cells? 
		- How do you divide the problem into subproblems 
		- What are the axes of the grid 
		- In dynamic programming you're trying to maximize something. 

	- code for matching 

	if word_a[i] == word_b[j]: # letters match  
		cell[i][j] = cell[i-1][j-1] #you decrement and take that letter out the need to match
	else: 
		cell[i][j] = 0 # the letters don't match 

	- note that the final solution might not always be int he last cell 

	- longest common subsequence 

	if word_a[i] == word_b[j]: 
		cell[i][j = cell[i-1][j-1] + 1 #the letters match 
	else: #the letters don't match
		cell[i][j] = max(cell[i-1][j], cell[i[j-1]])

	Aka if the letters don't match pick the larger, if they do match, this value is value of the top left neighbor + 1 


Chapter 10 

- k-nearest neighbors 

	- Looking at nearest neighbors to a data point to determine if something is one thing or another.
	- KNN is an algorithm for classification 
	- If you're trying to classify something, try KNN frist 

	- Graphs for recommendations based on movie tastes will work similarly, for say users that like movies of a certain genre, then another user that tends to like the same genre, will also like a movie. 

	- Feature extraction 
		- Features are the aspects you'll use to compare things. For grapefruits and oranges it's the redness, and size, on a graph, from 1 to 5 respectively. 
		- This can also be used for users, as a rating system on how much they like categories or movies. 
			- you can then use the pythagorean formula to calculate the difference between people or fruits 
		- To see how she might rank something you'll take a random sampling of people from around a person, and take their average, which is called regression 

	- Intro to machine learning 
		- OCR stands for optical character recognition 
		- OCR algorithms measure lines, points, and curves to look for similar images that they've stored, such as a 3, a 7, etc. 
		- Machine learning algorithms have training steps where they must be trained before the task, where it's shown that it has to do. 
		- Spam Filters are an example as well, and are built on the Naive Bayes Classifier. 
		